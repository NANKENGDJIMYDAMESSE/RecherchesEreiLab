{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jedt48y6J1dV",
        "outputId": "d54b838e-76ac-4182-b866-d3bcc4488cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 485kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.34MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transformation : Normalisation simple [0,1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Chargement du dataset MNIST\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Dataloader\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def poisson_encode(img, time_steps=20):\n",
        "    \"\"\" Encode une image en train de Poisson binaire pour SNN \"\"\"\n",
        "    img = img.view(-1, 28, 28)\n",
        "    spike_train = torch.rand((time_steps, *img.shape)) < img.unsqueeze(0)\n",
        "    return spike_train.float()\n"
      ],
      "metadata": {
        "id": "TAj-bvfMJ7gQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img, _ = train_dataset[0]\n",
        "encoded_spike = poisson_encode(sample_img, time_steps=20)\n",
        "print(encoded_spike.shape)  # (time_steps, 1, 28, 28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQTCuexaKDkV",
        "outputId": "6ad3de4d-6b08-465d-fb7a-0b8e7c8baf9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SparseSNN(nn.Module):\n",
        "    def __init__(self, input_size=28*28, hidden_size=500, output_size=10, sparsity=0.8):\n",
        "        super(SparseSNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.sparsity = sparsity\n",
        "\n",
        "    def forward(self, x_spike_train):\n",
        "        batch_size = x_spike_train.shape[1]\n",
        "        mem = torch.zeros(batch_size, self.fc2.out_features, device=x_spike_train.device)\n",
        "\n",
        "        for t in range(x_spike_train.shape[0]):\n",
        "            x = x_spike_train[t].view(batch_size, -1)\n",
        "\n",
        "            # Ajout de sparsité artificielle\n",
        "            mask = (torch.rand_like(x) > self.sparsity).float()\n",
        "            x = x * mask\n",
        "\n",
        "            hidden = torch.relu(self.fc1(x))\n",
        "            output = self.fc2(hidden)\n",
        "\n",
        "            mem += output\n",
        "\n",
        "        return mem / x_spike_train.shape[0]\n"
      ],
      "metadata": {
        "id": "u6jxXl-mKDhX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stdp_update(pre_spikes, post_spikes, weights, lr=1e-3):\n",
        "    \"\"\" Mise à jour locale des poids avec règle Hebbienne simplifiée \"\"\"\n",
        "    delta_w = torch.bmm(post_spikes.unsqueeze(2), pre_spikes.unsqueeze(1))\n",
        "    weights.data += lr * delta_w.mean(dim=0)\n",
        "    weights.data = torch.clamp(weights.data, -1.0, 1.0)\n"
      ],
      "metadata": {
        "id": "IoELlCedKGLY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SparseSNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 5\n",
        "time_steps = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        encoded = poisson_encode(imgs, time_steps=time_steps).to(device)\n",
        "        outputs = model(encoded)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f} - Accuracy: {correct/len(train_dataset):.4f} - Time: {end_time-start_time:.2f}s')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHymgHmaKGIb",
        "outputId": "a4cb0679-6817-4964-9784-20ed217dba17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 2123.9198 - Accuracy: 0.3773 - Time: 51.57s\n",
            "Epoch 2/5 - Loss: 1972.7196 - Accuracy: 0.5776 - Time: 49.59s\n",
            "Epoch 3/5 - Loss: 1589.4377 - Accuracy: 0.6837 - Time: 49.62s\n",
            "Epoch 4/5 - Loss: 1144.2993 - Accuracy: 0.7535 - Time: 52.22s\n",
            "Epoch 5/5 - Loss: 866.7774 - Accuracy: 0.7961 - Time: 49.46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "total_energy = 0\n",
        "total_time = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        start = time.perf_counter()\n",
        "        encoded = poisson_encode(imgs, time_steps=time_steps).to(device)\n",
        "        outputs = model(encoded)\n",
        "        end = time.perf_counter()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        test_correct += (preds == labels).sum().item()\n",
        "\n",
        "        total_time += (end - start)\n",
        "        # Simulation grossière de consommation énergétique\n",
        "        total_energy += (end - start) * 0.5  # (arbitraire : 0.5W utilisation estimée)\n",
        "\n",
        "print(f'Test Accuracy: {test_correct/len(test_dataset):.4f}')\n",
        "print(f'Test Inference Time (total): {total_time:.2f}s')\n",
        "print(f'Estimated Energy Consumed: {total_energy:.4f} Joules')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R16QDBfKGFQ",
        "outputId": "09cc5c33-8ed9-4d86-a4ca-356da4657294"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8165\n",
            "Test Inference Time (total): 4.38s\n",
            "Estimated Energy Consumed: 2.1878 Joules\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r3EZlCL4KGCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1ANdnOQKF_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST AVEC LES 3 DATASETS **"
      ],
      "metadata": {
        "id": "LQiAprEBLepH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Définition d'une fonction pour charger le dataset\n",
        "def load_dataset(name, batch_size=64):\n",
        "    \"\"\"\n",
        "    Charge le dataset spécifié (MNIST, Fashion-MNIST ou CIFAR-10)\n",
        "    \"\"\"\n",
        "    # Transformations : convertit en Tensor et normalise entre 0 et 1\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    if name == 'MNIST':\n",
        "        train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    elif name == 'FashionMNIST':\n",
        "        train_set = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        test_set = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    elif name == 'CIFAR10':\n",
        "        # Pour CIFAR-10 : normaliser couleurs entre 0 et 1\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Grayscale(num_output_channels=1),  # On passe en niveaux de gris pour être cohérent\n",
        "            transforms.Resize((28, 28)),  # Redimensionne à 28x28\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "        test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    else:\n",
        "        raise ValueError(\"Dataset non supporté : Choisir MNIST, FashionMNIST ou CIFAR10\")\n",
        "\n",
        "    # Chargement sous forme de DataLoader\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "oZJeQszsKF9F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poisson_encode(images, time_steps=20):\n",
        "    \"\"\"\n",
        "    Transforme des images continues en train d'impulsions binaires (spikes) sur plusieurs temps\n",
        "    \"\"\"\n",
        "    # images: (batch_size, 1, 28, 28)\n",
        "    images = images.view(images.size(0), -1)  # aplatissement images en vecteur (batch_size, 784)\n",
        "\n",
        "    # Génère des spikes selon une distribution de Bernoulli basée sur l'intensité\n",
        "    spike_train = torch.rand(time_steps, *images.shape).to(images.device) < images.unsqueeze(0)\n",
        "\n",
        "    # Retourne le train de spikes (time_steps, batch_size, 784)\n",
        "    return spike_train.float()\n"
      ],
      "metadata": {
        "id": "p8CdEB3CKF56"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SparseSNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Modèle simple de réseau de neurones à pics avec sparsité imposée\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=28*28, hidden_size=500, output_size=10, sparsity=0.8):\n",
        "        super(SparseSNN, self).__init__()\n",
        "\n",
        "        # Couche entièrement connectée entre input et hidden\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Couche entre hidden et output\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Taux de sparsité (0.8 => 80% des entrées sont éteintes à chaque temps)\n",
        "        self.sparsity = sparsity\n",
        "\n",
        "    def forward(self, spike_train):\n",
        "        \"\"\"\n",
        "        spike_train: (time_steps, batch_size, input_size)\n",
        "        \"\"\"\n",
        "        batch_size = spike_train.shape[1]\n",
        "\n",
        "        # Accumulateur de potentiel de membrane (sur les sorties)\n",
        "        membrane_potential = torch.zeros(batch_size, self.fc2.out_features, device=spike_train.device)\n",
        "\n",
        "        # Boucle temporelle\n",
        "        for t in range(spike_train.shape[0]):\n",
        "            x = spike_train[t]  # état à l'instant t\n",
        "            # Appliquer la sparsité (désactiver aléatoirement des neurones d'entrée)\n",
        "            mask = (torch.rand_like(x) > self.sparsity).float()\n",
        "            x_sparse = x * mask\n",
        "\n",
        "            # Passage dans réseau\n",
        "            hidden = torch.relu(self.fc1(x_sparse))\n",
        "            output = self.fc2(hidden)\n",
        "\n",
        "            # Accumulation du potentiel\n",
        "            membrane_potential += output\n",
        "\n",
        "        # Retourne moyenne sur tous les time steps\n",
        "        return membrane_potential / spike_train.shape[0]\n"
      ],
      "metadata": {
        "id": "SkuUKcDZKF3M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, device, time_steps=20):\n",
        "    \"\"\"\n",
        "    Entraînement du modèle SNN sur un epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Encodage des images en train de spikes\n",
        "        spike_train = poisson_encode(images, time_steps=time_steps)\n",
        "\n",
        "        # Passage du train de spikes dans le réseau\n",
        "        outputs = model(spike_train)\n",
        "\n",
        "        # Calcul de la perte\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Rétropropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / len(train_loader.dataset)\n",
        "    return total_loss, accuracy\n",
        "\n",
        "def test(model, test_loader, device, time_steps=20):\n",
        "    \"\"\"\n",
        "    Test du modèle SNN\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total_inference_time = 0.0\n",
        "    total_energy = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            start = torch.cuda.Event(enable_timing=True)\n",
        "            end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            spike_train = poisson_encode(images, time_steps=time_steps)\n",
        "\n",
        "            start.record()\n",
        "            outputs = model(spike_train)\n",
        "            end.record()\n",
        "\n",
        "            torch.cuda.synchronize()  # Synchroniser les événements CUDA\n",
        "            inference_time = start.elapsed_time(end) / 1000.0  # en secondes\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "            total_inference_time += inference_time\n",
        "            total_energy += inference_time * 0.5  # Consommation simulée: 0.5 Joules par seconde\n",
        "\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    return accuracy, total_inference_time, total_energy\n"
      ],
      "metadata": {
        "id": "eUqOkJU5KDd5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(dataset_name='MNIST', epochs=5, batch_size=64, time_steps=20):\n",
        "    \"\"\"\n",
        "    Fonction principale pour entraîner et tester un modèle SNN bio-inspiré\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Chargement des données\n",
        "    train_loader, test_loader = load_dataset(dataset_name, batch_size=batch_size)\n",
        "\n",
        "    # Initialisation du modèle\n",
        "    model = SparseSNN().to(device)\n",
        "\n",
        "    # Définition de l'optimiseur et de la fonction de perte\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        loss, train_acc = train(model, train_loader, optimizer, criterion, device, time_steps=time_steps)\n",
        "        print(f\"Train Loss: {loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    # Test final\n",
        "    test_acc, inference_time, energy = test(model, test_loader, device, time_steps=time_steps)\n",
        "    print(f\"\\n--- Résultats Test sur {dataset_name} ---\")\n",
        "    print(f\"Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Temps d'inférence total: {inference_time:.2f} secondes\")\n",
        "    print(f\"Énergie estimée: {energy:.2f} Joules\")\n",
        "\n",
        "# Exemple d'utilisation\n",
        "main('MNIST')\n",
        "# main('FashionMNIST')\n",
        "# main('CIFAR10')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ma6oY3AKDaL",
        "outputId": "6e36012a-1125-433b-9ed8-68b943f4b821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKuvw21BKDWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xnuPjkDKDTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jp1HL1YXKDQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-2nHfQ8KDNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0Jc1AlIKDLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iB5MFDedKDIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rq1IMhrWKDF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GW5uy5L9KDDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnouDxA2KDA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQZrBTqLKC8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44DHzY2EKC6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3n3BW1KKC3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZVDQHBQKC1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e0njmxGeKCzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PzFrNkt3KCxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U37ick7NKCu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7INSyd6KCsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DqqYfdmqKCqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}